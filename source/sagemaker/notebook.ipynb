{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Explaining Credit Decisions"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Given the increasing complexity of machine learning models, the need for model\n", "explainability has been growing lately. Some governments have also introduced\n", "stricter regulations that mandate a *right to explanation* from machine\n", "learning models. In this solution, we take a look at how [Amazon\n", "SageMaker](https://aws.amazon.com/sagemaker/) can be used to explain\n", "individual predictions from machine learning models.\n", "\n", "As an example application, we classify credit applications and predict whether\n", "the credit would be payed back or not (often called a *credit default*). Given\n", "a credit application from a bank customer, the aim of the bank is to predict\n", "whether or not the customer will pay back the credit in accordance with their\n", "repayment plan. When a customer can't pay back their credit, often called a\n", "'default', the bank loses money and the customers credit score will be\n", "impacted. On the other hand, denying trustworthy customers credit also has a\n", "set of negative impacts. Using accurate machine learning models to classify\n", "the risk of a credit application can help find a good balance between these\n", "two scenarios, but this provides no comfort to those customers who have been\n", "denied credit. Using explanability methods, it's possible to determine\n", "actionable factors that had a negative impact on the application. Customers\n", "can then take action to increase their chance of obtaining credit in\n", "subsequent applications.\n", "\n", "We train a tree-based [LightGBM](https://lightgbm.readthedocs.io/en/latest/)\n", "model using [Amazon SageMaker](https://aws.amazon.com/sagemaker/) and explain\n", "its predictions using a game theoretic approach called\n", "[SHAP](https://github.com/slundberg/shap) (SHapley Additive exPlanations). We\n", "deploy a endpoint that returns the credit default risk score, alongside an\n", "explanation."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Imports\n", "We start by importing a variety of packages that will be used throughout the\n", "notebook. One of the most important packages is the Amazon SageMaker Python\n", "SDK (i.e. `import sagemaker`). We also import modules from our own custom\n", "package that can be found at `./src/package`."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from bokeh.plotting import output_notebook\n", "import sagemaker\n", "from sagemaker.sklearn import SKLearn\n", "from sagemaker.local import LocalSession\n", "from sagemaker.predictor import json_serializer, json_deserializer, CONTENT_TYPE_JSON\n", "from pathlib import Path\n", "import boto3\n", "import shap\n", "import numpy as np\n", "import os\n", "\n", "from src.package import glue\n", "from src.package import config\n", "from src.package import schemas\n", "from src.package import datasets\n", "from src.package import containers\n", "from src.package import utils\n", "from src.package import visuals"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Datasets\n", "When creating the AWS CloudFormation stack, a collection of synthetic datasets\n", "were generated and stored in our solution Amazon S3 bucket with a prefix of\n", "`dataset`. Most of the features contained in these datasets are based on the\n", "[German Credit\n", "Dataset](http://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data))\n", "(UCI Machine Learning Repository), but there are some synthetic data fields\n", "too. All personal information was generated using\n", "[`Faker`](https://faker.readthedocs.io/en/master/). We have 3 datasets in\n", "total: credits, people and contacts."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Dataset #1: Credits\n", "\n", "Our credits dataset contains features directly related to the credit application.\n", "\n", "It is a CSV file (i.e. Comma Seperated Value file) that has a header row with feature names. Of particular note is the feature called `default`. It is our target variable that we're trying to predict with our LightGBM model. We show the first two rows of the dataset below:\n", "\n", "```\n", "\"credit_id\",\"person_id\",\"amount\",\"duration\",\"purpose\",\"installment_rate\",\"guarantor\",\"coapplicant\",\"default\"\n", "\"51829372\",\"f032303d\",1169,6,\"electronics\",4,0,0,False\n", "```"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Dataset #2: People\n", "\n", "Our credits data contains features related to the people making the credit applications (i.e. the applicants).\n", "\n", "It's a [JSON Lines](http://jsonlines.org/) file, where each row is a separate JSON object. Of particular note is the feature called `person_id`. You'll notice that this feature was also included in the credits dataset. It is used to connect the credit application with the applicant. We show the first row of the dataset below:  \n", "\n", "```\n", "{\n", "    \"person_id\": \"f032303d\",\n", "    \"finance\": {\n", "        \"accounts\": {\n", "            \"checking\": {\n", "                \"balance\": \"negative\"\n", "            }\n", "        },\n", "        \"repayment_history\": \"very_poor\",\n", "        \"credits\": {\n", "            \"this_bank\": 2,\n", "            \"other_banks\": 0,\n", "            \"other_stores\": 0\n", "        },\n", "        \"other_assets\": \"real_estate\"\n", "    },\n", "    \"personal\": {\n", "        \"age\": 67,\n", "        \"gender\": \"male\",\n", "        \"relationship_status\": \"single\",\n", "        \"name\": \"Peter Jones\"\n", "    },\n", "    \"dependents\": [\n", "        {\n", "            \"gender\": \"male\",\n", "            \"name\": \"Michael Morales\"\n", "        }\n", "    ],\n", "    \"employment\": {\n", "        \"type\": \"professional\",\n", "        \"title\": \"Learning disability nurse\",\n", "        \"duration\": 11,\n", "        \"permit\": \"foreign\"\n", "    },\n", "    \"residence\": {\n", "        \"type\": \"own\",\n", "        \"duration\": 4\n", "    }\n", "}\n", "```"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Dataset #3: Contacts\n", "\n", "Our contacts dataset contains contact information for the applicants.\n", "\n", "It is a CSV file that has a header row with feature names. Once again we have `person_id`. We show the first two rows of the dataset below:\n", "\n", "```\n", "\"contact_id\",\"person_id\",\"type\",\"value\"\n", "\"5996e20a\",\"f032303d\",\"telephone\",\"(716)406-9514x345\"\n", "```"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## AWS Glue\n", "\n", "One of the most time consuming tasks in developing a machine learning workflow\n", "is data preperation. AWS Glue can be used to simplify this process. As a\n", "demonstration of how it can be used to infer data schemas and perform extract,\n", "transform and load (ETL) jobs in Spark, we'll prepare dataset using AWS Glue.\n", "Although our sample datasets are small, there are many real world scenarios\n", "that will benefit from the scalability of AWS Glue.\n", "\n", "When creating the AWS CloudFormation stack, a number of AWS Glue resources\n", "were created:\n", "\n", "* A\n", "  [Database](https://docs.aws.amazon.com/glue/latest/dg/define-database.html)\n", "  is used to organize solution's tables.\n", "* A [Crawler](https://docs.aws.amazon.com/glue/latest/dg/add-crawler.html) is\n", "  used infer formats and schemas of the datasets above.\n", "* A [Custom\n", "  Classifier](https://docs.aws.amazon.com/glue/latest/dg/add-classifier.html)\n", "  is used to help the classifier infer the schema of the contacts datasets.\n", "    * All fields are of type 'string', so we need to indicate that the first\n", "      row is a header row rather than data.\n", "* A [Job](https://docs.aws.amazon.com/glue/latest/dg/author-job.html) is used\n", "  to join the datasets together, drop certain feature, create other features,\n", "  and split train and test sets.\n", "* A\n", "  [Workflow](https://docs.aws.amazon.com/glue/latest/dg/orchestrate-using-workflows.html)\n", "  (and associated\n", "  [triggers](https://docs.aws.amazon.com/glue/latest/dg/trigger-job.html)) to\n", "  orchestrate the above crawler and job.\n", "\n", "You can explore the service console for AWS Glue for more details, but for now\n", "we'll start the workflow. "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["glue_run_id = glue.start_workflow(config.GLUE_WORKFLOW)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Our workflow takes around 10 minutes to complete. Most of this time is spend\n", "on resource provisioning, but there is a [preview\n", "feature](https://pages.awscloud.com/glue-reduced-spark-times-preview-2020.html)\n", "for reduced start times. We'll wait until the AWS Glue workflow has completed\n", "before continuing. We need the dataset before training our model in Amazon\n", "SageMaker."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["glue.wait_for_workflow_finished(config.GLUE_WORKFLOW, glue_run_id)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["With our AWS Glue workflow complete, we should now have 4 additional datasets\n", "in our solution's Amazon S3 bucket: `data_train`, `label_train`, `data_test`\n", "and `label_test`. We show the first row of `data_train` below (although it may\n", "wrap onto two lines):\n", "\n", "```\n", "false,338,0,6,0,4,electronics,8,foreign,professional,negative,high,0,0,2,car_or_other,very_poor,52,male,1,single,4,own\n", "```\n", "\n", "We now have 23 features that describe a credit application and its applicant.\n", "We no longer have a header row of feature names, but fortunately all of this\n", "schema information is stored in our AWS Glue catalog. Since we're interested\n", "in explaining the model predictions, and our explanations attribute features,\n", "it's useful if our feature names are understandable.\n", "\n", "**Advanced**: We can also organize features in a hierarchy (using a seperator\n", "in the feature names), which enables summarization of the explanations. As an\n", "example, `employment__type` and `employment__duration` are both `employment`\n", "related features. We use two consecutive underscores (`__`) as our level\n", "separator."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Schema\n", "Schemas can be used to keep track of feature names, descriptions and types.\n", "Our solution uses\n", "[`jsonschema`](https://python-jsonschema.readthedocs.io/en/stable/) as the\n", "primary schema format. We have the added bonus of being able to use schemas to\n", "validate input to the trained model and deployed endpoints.\n", "\n", "We already have most of this schema information in our AWS Glue catalog, so\n", "let's start by retrieving the table schema for `data_train`."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["data_schema = glue.get_table_schema(\n", "    database_name=config.GLUE_DATABASE, table_name=\"data_train\"\n", ")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We can now add additional information such as feature descriptions, that will\n", "be shown inside the tooltip on the visuals later on."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# flake8: noqa: E501\n", "data_schema.title = \"Credit Application\"\n", "data_schema.description = \"An array of items used to describe a credit application.\"\n", "item_descriptions_dict = {\n", "    \"contact__has_telephone\": \"Customer has a registered telephone number.\",\n", "    \"credit__amount\": \"Amount of money requested as part of credit application (in EUR).\",\n", "    \"credit__coapplicant\": \"Co-applicant on credit application.\",\n", "    \"credit__duration\": \"Amount of time the credit is requested for (in months).\",\n", "    \"credit__guarantor\": \"Guarantor on credit application.\",\n", "    \"credit__installment_rate\": \"Credit installment rate (as a percentage of the customer's disposable income).\",\n", "    \"credit__purpose\": \"Customer's reason for requiring credit.\",\n", "    \"employment__duration\": \"Amount of time the customer has been employed at their current employer (in years).\",\n", "    \"employment__permit\": \"Customer's current work permit type.\",\n", "    \"employment__type\": \"Customer's current job classification.\",\n", "    \"finance__accounts__checking__balance\": \"Customer's checking account balance.\",\n", "    \"finance__accounts__savings__balance\": \"Customer's savings account balance.\",\n", "    \"finance__credits__other_banks\": \"Count of credits the customer has at other banks.\",\n", "    \"finance__credits__other_stores\": \"Count of credits the customer has at other stores.\",\n", "    \"finance__credits__this_bank\": \"Count of credits the customer has at this bank.\",\n", "    \"finance__other_assets\": \"Customer's most significant asset.\",\n", "    \"finance__repayment_history\": \"Quality of the customer's repayment history.\",\n", "    \"personal__age\": \"Customer's age in years.\",\n", "    \"personal__gender\": \"Customer's gender.\",\n", "    \"personal__num_dependents\": \"Count of the customer's dependents.\",\n", "    \"personal__relationship_status\": \"Customer's relationship status.\",\n", "    \"residence__duration\": \"Amount of time the customer has been at their current residence (in years).\",\n", "    \"residence__type\": \"Class of the customer's residence.\"\n", "}\n", "data_schema.item_descriptions_dict = item_descriptions_dict"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We do the same for `label_train` too."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["label_schema = glue.get_table_schema(\n", "    database_name=config.GLUE_DATABASE, table_name=\"label_train\"\n", ")\n", "label_schema.title = \"Credit Application Outcome\"\n", "item_descriptions_dict = {\n", "    \"credit__default\": (\n", "        \"0 if the customer successfully made credit payments, \"\n", "        \"1 if the customer defaulted on credit payments.\")\n", "}\n", "label_schema.item_descriptions_dict = item_descriptions_dict"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Since the schemas for train and test datasets are the same, we can skip\n", "`data_test` and `label_test`.\n", "\n", "We can save our updated schemas to disk, in preperation for uploading to\n", "Amazon S3."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["current_folder = utils.get_current_folder(globals())\n", "schema_folder = Path(current_folder, \"schemas\")\n", "data_schema_filepath = Path(schema_folder, \"data.schema.json\")\n", "data_schema.save(data_schema_filepath)\n", "label_schema_filepath = Path(schema_folder, \"label.schema.json\")\n", "label_schema.save(label_schema_filepath)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Up next, we create a SageMaker Session. A SageMaker Session can be used to\n", "conveniently perform certain AWS actions, such as uploading and downloading\n", "files from Amazon S3. We use the SageMaker Session to upload our schemas to\n", "Amazon S3."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["boto_session = boto3.session.Session(region_name=config.AWS_REGION)\n", "sagemaker_session = sagemaker.Session(boto_session)\n", "\n", "sagemaker_session.upload_data(\n", "    path=str(schema_folder),\n", "    bucket=config.S3_BUCKET,\n", "    key_prefix=config.SCHEMAS_S3_PREFIX\n", ")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Container\n", "We now build our custom Docker image that will be used for model training and\n", "deployment. It extends the official Amazon SageMaker framework image for\n", "Scikit-learn, by adding additional packages such as\n", "[LightGBM](https://lightgbm.readthedocs.io/en/latest/) and\n", "[SHAP](https://github.com/slundberg/shap). After building the image, we upload\n", "it to our solution's Amazon ECR repository."]}, {"cell_type": "code", "execution_count": null, "metadata": {"lines_to_next_cell": 2}, "outputs": [], "source": ["scikit_learn_image = containers.scikit_learn_image()\n", "custom_image = containers.custom_image()\n", "\n", "dockerfile = Path(current_folder, 'containers/Dockerfile')\n", "custom_image.build(\n", "    dockerfile=dockerfile,\n", "    buildargs={'SCIKIT_LEARN_IMAGE': str(scikit_learn_image)}\n", ")\n", "custom_image.push()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Model Training\n", "Amazon SageMaker provides two methods to training and deploying models. You\n", "can start by quickly testing and debuging models on the Amazon SageMaker\n", "Notebook instance using Local Mode (set `local = True`). After this, you can\n", "scale up training with SageMaker Mode on dedicated instances and deploy the\n", "model on dedicated instance too (set `local = False`). Since this is a\n", "pre-developed solution we'll start with SageMaker Mode."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["local = False\n", "if local:\n", "    train_instance_type = 'local'\n", "    deploy_instance_type = 'local'\n", "    session = LocalSession(boto_session)\n", "else:\n", "    train_instance_type = 'ml.c5.xlarge'\n", "    deploy_instance_type = 'ml.c5.xlarge'\n", "    session = sagemaker_session"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Up next, we configure our SKLearn estimator. We will use it to coordinate\n", "model training and deployment. We reference our custom container (see\n", "`image_name`) and our custom code (see `entry_point` and `source_dir`). At\n", "this stage, we also reference the instance type (and instance count) that will\n", "be used during training, and the hyperparmeters we wish to use. And lastly we\n", "set the `output_path` for trained model artifacts and `code_location` for a\n", "snapshot of the training script that was used."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["hyperparameters = {\n", "    \"tree-n-estimators\": 42,\n", "    \"tree-max-depth\": 2,\n", "    \"tree-min-child-samples\": 1,\n", "    \"tree-boosting-type\": \"dart\"\n", "}\n", "\n", "estimator = sagemaker.sklearn.SKLearn(\n", "    image_name=str(custom_image),\n", "    source_dir=str(Path(current_folder, 'src').resolve()),\n", "    entry_point='entry_point_explanations.py',\n", "    hyperparameters=hyperparameters,\n", "    role=config.SAGEMAKER_IAM_ROLE,\n", "    train_instance_count=1,\n", "    train_instance_type=train_instance_type,\n", "    sagemaker_session=session,\n", "    output_path='s3://' + str(Path(config.S3_BUCKET, config.OUTPUTS_S3_PREFIX)),\n", "    code_location='s3://' + str(Path(config.S3_BUCKET, config.OUTPUTS_S3_PREFIX))\n", ")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["With our estimator now initialized, we can start the Amazon SageMaker training\n", "job. Since our entry point script expects a number of data channels to be\n", "defined, we can provide them when calling `fit`. When referencing `s3://`\n", "folders, the contents of these folders will be automatically downloaded from\n", "Amazon S3 before the entry point script is run. When using local mode, it's\n", "possible to avoid this data transfer and reference local folder using the\n", "`file://` prefix instead: e.g. `{'schemas': 'file://' + str(schema_folder)}`\n", "\n", "You can expect this step to take approximately 5 minutes."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["estimator.fit({\n", "    'schemas': 's3://' + str(Path(config.S3_BUCKET, config.SCHEMAS_S3_PREFIX)),\n", "    'data_train': 's3://' + str(Path(config.S3_BUCKET, config.DATASETS_S3_PREFIX, 'data_train')),\n", "    'label_train': 's3://' + str(Path(config.S3_BUCKET, config.DATASETS_S3_PREFIX, 'label_train')),\n", "    'data_test': 's3://' + str(Path(config.S3_BUCKET, config.DATASETS_S3_PREFIX, 'data_test')),\n", "    'label_test': 's3://' + str(Path(config.S3_BUCKET, config.DATASETS_S3_PREFIX, 'label_test'))\n", "})"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Model Deployment\n", "Our Amazon SageMaker training job has now completed, and we should have a\n", "number of trained model artifacts that can be deployed. Calling `deploy` will\n", "start a container to host the model (and an instance to run the container if\n", "you're not running in local mode). Using `estimator.deploy` means that we'll\n", "use the same entry point script as used for training, but the model deployment\n", "functions (i.e. `model_fn`, `input_fn`, `predict_fn`, etc) will be used\n", "instead of the model training function (i.e. `train_fn`)."]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Caution**: When using local mode, you may see `docker-compose` errors if\n", "trying to deploy the estimator multiple times. You need to manually stop the\n", "original hosting container before deploying a second time. Uncomment and\n", "execute the following command to stop the original hosting container."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# !docker container stop $(docker ps -a -q --filter ancestor={config.ECR_REPOSITORY})"]}, {"cell_type": "markdown", "metadata": {}, "source": ["You can expect this step to take approximately 5 minutes.\n", "Note: AWS CloudFormation will delete this endpoint (and endpoint\n", "configuration) during stack deletion if the `endpoint_name` is kept as is. You\n", "will need to manually delete the endpoint (and endpoint configuration) after\n", "stack deletion if you change this."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["explainer = estimator.deploy(\n", "    endpoint_name=\"{}-endpoint\".format(config.STACK_NAME),\n", "    instance_type=deploy_instance_type,\n", "    initial_instance_count=1\n", ")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["When calling the `explainer` endpoint from the notebook, we first need to\n", "convert the features stored as a Python list into a JSON string. With the\n", "Amazon SageMaker Python SDK, we can simply set the `serializer` to the\n", "in-built `json_serializer`. Additionally, we notify to the endpoint that the\n", "contents being sent is in-fact JSON by setting the `content_type`. Similarly,\n", "we can request a response from the endpoint in JSON format by setting\n", "`accept`. And lastly, we can convert the JSON responce back into Python\n", "objects by setting `deserializer` to `json_deserializer`. \n", "\n", "You should be aware that these changes only effect endpoints calls from this\n", "notebook."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["explainer.serializer = json_serializer\n", "explainer.content_type = CONTENT_TYPE_JSON  # 'application/json'\n", "explainer.accept = CONTENT_TYPE_JSON        # 'application/json'\n", "explainer.deserializer = json_deserializer"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Model Explanations"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# data_train = datasets.read_csv_dataset(Path(datasets_folder, 'data_train'), data_schema)\n", "# sample = data_train[0, :].tolist()\n", "sample = {\n", "    'contact__has_telephone': False,\n", "    'credit__amount': 433,\n", "    'credit__coapplicant': 1,\n", "    'credit__duration': 18,\n", "    'credit__guarantor': 0,\n", "    'credit__installment_rate': 3,\n", "    'credit__purpose': 'electronics',\n", "    'employment__duration': 0,\n", "    'employment__permit': 'foreign',\n", "    'employment__type': 'professional',\n", "    'finance__accounts__checking__balance': 'no_account',\n", "    'finance__accounts__savings__balance': 'low',\n", "    'finance__credits__other_banks': 0,\n", "    'finance__credits__other_stores': 0,\n", "    'finance__credits__this_bank': 1,\n", "    'finance__other_assets': 'real_estate',\n", "    'finance__repayment_history': 'good',\n", "    'personal__age': 22,\n", "    'personal__gender': 'male',\n", "    'personal__num_dependents': 1,\n", "    'personal__relationship_status': 'married',\n", "    'residence__duration': 4,\n", "    'residence__type': 'rent'\n", "}"]}, {"cell_type": "markdown", "metadata": {"lines_to_next_cell": 2}, "source": ["When calling `predict`, the `sample` will be serialized and sent to the\n", "`explainer` endpoint. Our endpoint, after running the model deployment\n", "functions in the entry point, will return a predicted probability of credit\n", "default and an explanation for this prediction.\n", "\n", "**Caution**: the probability returned by this model has not been calibrated.\n", "When the model gives a probability of credit default of 20%, for example, this\n", "does not necessarily mean that 20% of applications with a probability of 20%\n", "resulted in credit default. Calibration is a useful property in certain\n", "circumstances, but is not required in cases where discrimination between cases\n", "of default and non-defult is sufficient.\n", "[CalibratedClassifierCV](https://scikit-learn.org/stable/modules/generated/sklearn.calibration.CalibratedClassifierCV.html)\n", "from [Scikit-learn](https://scikit-learn.org/stable/modules/calibration.html)\n", "can be used to calibrate a model. Calibration also has an impact on the\n", "explanations. Since the calibration process is typically non-linear, it breaks\n", "the additive property of Shapley Values.\n", "[`KernelExplainer`](https://shap.readthedocs.io/en/latest/) can handle this\n", "case, but is typically much slower to compute the explanations."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["output = explainer.predict(sample)\n", "prediction = output['prediction']\n", "print(\"prediction: {:.2%}\".format(prediction))\n", "explanation = output['explanation']"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Visualizing Explanations"]}, {"cell_type": "code", "execution_count": null, "metadata": {"lines_to_next_cell": 0}, "outputs": [], "source": ["output_notebook()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Summary Explanation"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["explanation_summary = visuals.summarize_explanation(explanation)\n", "summary_waterfall = visuals.WaterfallChart(\n", "    baseline=explanation_summary['expected_value'],\n", "    shap_values=explanation_summary['shap_values'],\n", "    names=explanation_summary['feature_names'],\n", "    descriptions=explanation_summary['feature_descriptions'],\n", "    max_features=10,\n", "    x_axis_label='Credit Default Risk Score (%)',\n", ")\n", "summary_waterfall.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Detailed Explanation"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["detailed_waterfall = visuals.WaterfallChart(\n", "    baseline=explanation['expected_value'],\n", "    shap_values=explanation['shap_values'],\n", "    names=explanation['feature_names'],\n", "    feature_values=explanation['feature_values'],\n", "    descriptions=explanation['feature_descriptions'],\n", "    max_features=10,\n", "    x_axis_label='Credit Default Risk Score (%)'\n", ")\n", "detailed_waterfall.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Counterfactual Example"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["sample['finance__accounts__checking__balance'] = 'negative'  # from 'no_account'\n", "explanation = explainer.predict(sample)['explanation']\n", "detailed_waterfall = visuals.WaterfallChart(\n", "    baseline=explanation['expected_value'],\n", "    shap_values=explanation['shap_values'],\n", "    names=explanation['feature_names'],\n", "    feature_values=explanation['feature_values'],\n", "    descriptions=explanation['feature_descriptions'],\n", "    max_features=10,\n", "    x_axis_label='Credit Default Risk Score (%)',\n", ")\n", "detailed_waterfall.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Clean Up\n", "When you've finished with this solution, make sure that you delete all\n", "unwanted AWS resources. AWS CloudFormation can be used to automatically delete\n", "all standard resources that have been created by the solution and notebook.\n", "\n", "**Caution**: You need to manually delete any extra resources that you may have\n", "created in this notebook. Some examples include, extra Amazon S3 buckets (to\n", "the solution's default bucket), extra Amazon SageMaker endpoints (using a\n", "custom name), and extra Amazon ECR repositories."]}, {"cell_type": "markdown", "metadata": {}, "source": ["You can explicitly delete the Amazon SageMaker endpoint (and endpoint configuration)\n", "using the Amazon SageMaker Python SDK, but this is also deleted in the AWS\n", "CloudFormation stack if you forget."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# explainer.delete_endpoint()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["You can now return to AWS CloudFormation and delete the stack."]}], "metadata": {"jupytext": {"cell_metadata_filter": "-all", "main_language": "python", "notebook_metadata_filter": "-all"}, "kernelspec": {"display_name": "conda_python3", "language": "python", "name": "conda_python3"}}, "nbformat": 4, "nbformat_minor": 4}