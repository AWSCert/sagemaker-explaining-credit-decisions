{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Explaining Credit Decisions"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Given the increasing complexity of machine learning models, the need for model\n", "explainability has been growing lately. Some governments have also introduced\n", "stricter regulations that mandate a *right to explanation* from machine\n", "learning models. In this solution, we take a look at how [Amazon\n", "SageMaker](https://aws.amazon.com/sagemaker/) can be used to explain\n", "individual predictions from machine learning models.\n", "\n", "As an example application, we classify credit applications and predict whether\n", "the credit would be payed back or not (often called a *credit default*). Given\n", "a credit application from a bank customer, the aim of the bank is to predict\n", "whether or not the customer will pay back the credit in accordance with their\n", "repayment plan. When a customer can't pay back their credit, often called a\n", "'default', the bank loses money and the customers credit score will be\n", "impacted. On the other hand, denying trustworthy customers credit also has a\n", "set of negative impacts. Using accurate machine learning models to classify\n", "the risk of a credit application can help find a good balance between these\n", "two scenarios, but this provides no comfort to those customers who have been\n", "denied credit. Using explanability methods, it's possible to determine\n", "actionable factors that had a negative impact on the application. Customers\n", "can then take action to increase their chance of obtaining credit in\n", "subsequent applications.\n", "\n", "We train a tree-based [LightGBM](https://lightgbm.readthedocs.io/en/latest/)\n", "model using [Amazon SageMaker](https://aws.amazon.com/sagemaker/) and explain\n", "its predictions using a game theoretic approach called\n", "[SHAP](https://github.com/slundberg/shap) (SHapley Additive exPlanations). We\n", "deploy a endpoint that returns the credit default risk score, alongside an\n", "explanation."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Imports\n", "We start by importing a variety of packages that will be used throughout the\n", "notebook. One of the most important packages is the Amazon SageMaker Python\n", "SDK (i.e. `import sagemaker`). We also import modules from our own custom\n", "package that can be found at `./src/package`."]}, {"cell_type": "code", "execution_count": null, "metadata": {"lines_to_next_cell": 2}, "outputs": [], "source": ["from bokeh.plotting import output_notebook\n", "import boto3\n", "from IPython.display import HTML\n", "import numpy as np\n", "import os\n", "from pathlib import Path\n", "import sagemaker\n", "from sagemaker.sklearn import SKLearn, SKLearnModel\n", "from sagemaker.local import LocalSession\n", "import shap\n", "\n", "from package import config, utils, visuals, reports\n", "from package.data import glue, datasets, schemas\n", "from package.sagemaker import containers, predictor_fns, predictors"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Datasets\n", "When creating the AWS CloudFormation stack, a collection of synthetic datasets\n", "were generated and stored in our solution Amazon S3 bucket with a prefix of\n", "`dataset`. Most of the features contained in these datasets are based on the\n", "[German Credit\n", "Dataset](http://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data))\n", "(UCI Machine Learning Repository), but there are some synthetic data fields\n", "too. All personal information was generated using\n", "[`Faker`](https://faker.readthedocs.io/en/master/). We have 3 datasets in\n", "total: credits, people and contacts."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Dataset #1: Credits\n", "\n", "Our credits dataset contains features directly related to the credit application.\n", "\n", "It is a CSV file (i.e. Comma Seperated Value file) that has a header row with feature names. Of particular note is the feature called `default`. It is our target variable that we're trying to predict with our LightGBM model. We show the first two rows of the dataset below:\n", "\n", "```\n", "\"credit_id\",\"person_id\",\"amount\",\"duration\",\"purpose\",\"installment_rate\",\"guarantor\",\"coapplicant\",\"default\"\n", "\"51829372\",\"f032303d\",1169,6,\"electronics\",4,0,0,False\n", "```"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Dataset #2: People\n", "\n", "Our credits data contains features related to the people making the credit applications (i.e. the applicants).\n", "\n", "It's a [JSON Lines](http://jsonlines.org/) file, where each row is a separate JSON object. Of particular note is the feature called `person_id`. You'll notice that this feature was also included in the credits dataset. It is used to connect the credit application with the applicant. We show the first row of the dataset below:  \n", "\n", "```\n", "{\n", "    \"person_id\": \"f032303d\",\n", "    \"finance\": {\n", "        \"accounts\": {\n", "            \"checking\": {\n", "                \"balance\": \"negative\"\n", "            }\n", "        },\n", "        \"repayment_history\": \"very_poor\",\n", "        \"credits\": {\n", "            \"this_bank\": 2,\n", "            \"other_banks\": 0,\n", "            \"other_stores\": 0\n", "        },\n", "        \"other_assets\": \"real_estate\"\n", "    },\n", "    \"personal\": {\n", "        \"age\": 67,\n", "        \"gender\": \"male\",\n", "        \"relationship_status\": \"single\",\n", "        \"name\": \"Peter Jones\"\n", "    },\n", "    \"dependents\": [\n", "        {\n", "            \"gender\": \"male\",\n", "            \"name\": \"Michael Morales\"\n", "        }\n", "    ],\n", "    \"employment\": {\n", "        \"type\": \"professional\",\n", "        \"title\": \"Learning disability nurse\",\n", "        \"duration\": 11,\n", "        \"permit\": \"foreign\"\n", "    },\n", "    \"residence\": {\n", "        \"type\": \"own\",\n", "        \"duration\": 4\n", "    }\n", "}\n", "```"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Dataset #3: Contacts\n", "\n", "Our contacts dataset contains contact information for the applicants.\n", "\n", "It is a CSV file that has a header row with feature names. Once again we have `person_id`. We show the first two rows of the dataset below:\n", "\n", "```\n", "\"contact_id\",\"person_id\",\"type\",\"value\"\n", "\"5996e20a\",\"f032303d\",\"telephone\",\"(716)406-9514x345\"\n", "```"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## AWS Glue\n", "\n", "One of the most time consuming tasks in developing a machine learning workflow\n", "is data preperation. AWS Glue can be used to simplify this process. As a\n", "demonstration of how it can be used to infer data schemas and perform extract,\n", "transform and load (ETL) jobs in Spark, we'll prepare dataset using AWS Glue.\n", "Although our sample datasets are small, there are many real world scenarios\n", "that will benefit from the scalability of AWS Glue.\n", "\n", "When creating the AWS CloudFormation stack, a number of AWS Glue resources\n", "were created:\n", "\n", "* A\n", "  [Database](https://docs.aws.amazon.com/glue/latest/dg/define-database.html)\n", "  is used to organize solution's tables.\n", "* A [Crawler](https://docs.aws.amazon.com/glue/latest/dg/add-crawler.html) is\n", "  used infer formats and schemas of the datasets above.\n", "* A [Custom\n", "  Classifier](https://docs.aws.amazon.com/glue/latest/dg/add-classifier.html)\n", "  is used to help the classifier infer the schema of the contacts datasets.\n", "    * All fields are of type 'string', so we need to indicate that the first\n", "      row is a header row rather than data.\n", "* A [Job](https://docs.aws.amazon.com/glue/latest/dg/author-job.html) is used\n", "  to join the datasets together, drop certain feature, create other features,\n", "  and split train and test sets.\n", "* A\n", "  [Workflow](https://docs.aws.amazon.com/glue/latest/dg/orchestrate-using-workflows.html)\n", "  (and associated\n", "  [triggers](https://docs.aws.amazon.com/glue/latest/dg/trigger-job.html)) to\n", "  orchestrate the above crawler and job.\n", "\n", "You can explore the service console for AWS Glue for more details, but for now\n", "we'll start the workflow. "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["glue_run_id = glue.start_workflow(config.GLUE_WORKFLOW)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Our workflow takes around 10 minutes to complete. Most of this time is spend\n", "on resource provisioning, but there is a [preview\n", "feature](https://pages.awscloud.com/glue-reduced-spark-times-preview-2020.html)\n", "for reduced start times. We'll wait until the AWS Glue workflow has completed\n", "before continuing. We need the dataset before training our model in Amazon\n", "SageMaker."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["glue.wait_for_workflow_finished(config.GLUE_WORKFLOW, glue_run_id)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["With our AWS Glue workflow complete, we should now have 4 additional\n", "datasets in our solution's Amazon S3 bucket: `data_train`, `label_train`,\n", "`data_test` and `label_test`. We show an example first row of\n", "`data_train` below (although it may wrap onto two lines):\n", "\n", "```\n", "false,276,0,9,0,4,new_car,3,foreign,labourer,low,low,0,0,1,real_estate,good,1,4,rent\n", "```\n", "\n", "We now have 20 features that describe a credit application and its applicant.\n", "We no longer have a header row of feature names, but fortunately all of this\n", "schema information is stored in our AWS Glue catalog. Since we're interested\n", "in explaining the model predictions, and our explanations attribute features,\n", "it's useful if our feature names are understandable.\n", "\n", "**Advanced**: We can also organize features in a hierarchy (using a seperator\n", "in the feature names), which enables summarization of the explanations. As an\n", "example, `employment__type` and `employment__duration` are both `employment`\n", "related features. We use two consecutive underscores (`__`) as our level\n", "separator."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Schema\n", "Schemas can be used to keep track of feature names, descriptions and types.\n", "Our solution uses\n", "[`jsonschema`](https://python-jsonschema.readthedocs.io/en/stable/) as the\n", "primary schema format. We have the added bonus of being able to use schemas to\n", "validate input to the trained model and deployed endpoints.\n", "\n", "We already have most of this schema information in our AWS Glue catalog, so\n", "let's start by retrieving the table schema for `data_train`."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["data_schema = glue.get_table_schema(\n", "    database_name=config.GLUE_DATABASE, table_name=\"data_train\"\n", ")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We can now add additional information such as feature descriptions, that will\n", "be shown inside the tooltip on the visuals later on."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# flake8: noqa: E501\n", "data_schema.title = \"Credit Application\"\n", "data_schema.description = \"An array of items used to describe a credit application.\"\n", "item_descriptions_dict = {\n", "    \"contact__has_telephone\": \"Customer has a registered telephone number.\",\n", "    \"credit__amount\": \"Amount of money requested as part of credit application (in EUR).\",\n", "    \"credit__coapplicant\": \"Co-applicant on credit application.\",\n", "    \"credit__duration\": \"Amount of time the credit is requested for (in months).\",\n", "    \"credit__guarantor\": \"Guarantor on credit application.\",\n", "    \"credit__installment_rate\": \"Credit installment rate (as a percentage of the customer's disposable income).\",\n", "    \"credit__purpose\": \"Customer's reason for requiring credit.\",\n", "    \"employment__duration\": \"Amount of time the customer has been employed at their current employer (in years).\",\n", "    \"employment__permit\": \"Customer's current work permit type.\",\n", "    \"employment__type\": \"Customer's current job classification.\",\n", "    \"finance__accounts__checking__balance\": \"Customer's checking account balance.\",\n", "    \"finance__accounts__savings__balance\": \"Customer's savings account balance.\",\n", "    \"finance__credits__other_banks\": \"Count of credits the customer has at other banks.\",\n", "    \"finance__credits__other_stores\": \"Count of credits the customer has at other stores.\",\n", "    \"finance__credits__this_bank\": \"Count of credits the customer has at this bank.\",\n", "    \"finance__other_assets\": \"Customer's most significant asset.\",\n", "    \"finance__repayment_history\": \"Quality of the customer's repayment history.\",\n", "    \"personal__num_dependents\": \"Count of the customer's dependents.\",\n", "    \"residence__duration\": \"Amount of time the customer has been at their current residence (in years).\",\n", "    \"residence__type\": \"Class of the customer's residence.\"\n", "}\n", "data_schema.item_descriptions_dict = item_descriptions_dict"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We do the same for `label_train` too."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["label_schema = glue.get_table_schema(\n", "    database_name=config.GLUE_DATABASE, table_name=\"label_train\"\n", ")\n", "label_schema.title = \"Credit Application Outcome\"\n", "item_descriptions_dict = {\n", "    \"credit__default\": (\n", "        \"0 if the customer successfully made credit payments, \"\n", "        \"1 if the customer defaulted on credit payments.\")\n", "}\n", "label_schema.item_descriptions_dict = item_descriptions_dict"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Since the schemas for train and test datasets are the same, we can skip\n", "`data_test` and `label_test`.\n", "\n", "We can save our updated schemas to disk, in preperation for uploading to\n", "Amazon S3."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["current_folder = utils.get_current_folder(globals())\n", "schema_folder = Path(current_folder, \"schemas\")\n", "data_schema_filepath = Path(schema_folder, \"data.schema.json\")\n", "data_schema.save(data_schema_filepath)\n", "label_schema_filepath = Path(schema_folder, \"label.schema.json\")\n", "label_schema.save(label_schema_filepath)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Up next, we create a SageMaker Session. A SageMaker Session can be used to\n", "conveniently perform certain AWS actions, such as uploading and downloading\n", "files from Amazon S3. We use the SageMaker Session to upload our schemas to\n", "Amazon S3."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["boto_session = boto3.session.Session(region_name=config.AWS_REGION)\n", "sagemaker_session = sagemaker.Session(boto_session)\n", "\n", "sagemaker_session.upload_data(\n", "    path=str(schema_folder),\n", "    bucket=config.S3_BUCKET,\n", "    key_prefix=config.SCHEMAS_S3_PREFIX\n", ")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Container\n", "We now build our custom Docker image that will be used for model training and\n", "deployment. It extends the official Amazon SageMaker framework image for\n", "Scikit-learn, by adding additional packages such as\n", "[LightGBM](https://lightgbm.readthedocs.io/en/latest/) and\n", "[SHAP](https://github.com/slundberg/shap). After building the image, we upload\n", "it to our solution's Amazon ECR repository."]}, {"cell_type": "code", "execution_count": null, "metadata": {"lines_to_next_cell": 2}, "outputs": [], "source": ["scikit_learn_image = containers.scikit_learn_image()\n", "custom_image = containers.custom_image()\n", "\n", "dockerfile = Path(current_folder, 'containers/Dockerfile')\n", "custom_image.build(\n", "    dockerfile=dockerfile,\n", "    buildargs={'SCIKIT_LEARN_IMAGE': str(scikit_learn_image)}\n", ")\n", "custom_image.push()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Model Training\n", "Amazon SageMaker provides two methods to training and deploying models. You\n", "can start by quickly testing and debuging models on the Amazon SageMaker\n", "Notebook instance using Local Mode (set `local = True`). After this, you can\n", "scale up training with SageMaker Mode on dedicated instances and deploy the\n", "model on dedicated instance too (set `local = False`). Since this is a\n", "pre-developed solution we'll start with SageMaker Mode."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["local = False\n", "if local:\n", "    train_instance_type = 'local'\n", "    deploy_instance_type = 'local'\n", "    session = LocalSession(boto_session)\n", "else:\n", "    train_instance_type = 'ml.c5.xlarge'\n", "    deploy_instance_type = 'ml.c5.xlarge'\n", "    session = sagemaker_session"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Up next, we configure our SKLearn estimator. We will use it to coordinate\n", "model training and deployment. We reference our custom container (see\n", "`image_name`) and our custom code (see `entry_point` and `source_dir`). At\n", "this stage, we also reference the instance type (and instance count) that will\n", "be used during training, and the hyperparmeters we wish to use. And lastly we\n", "set the `output_path` for trained model artifacts and `code_location` for a\n", "snapshot of the training script that was used.\n", "\n", "**Note**: when customizing the solution, you can enable enhanced logging\n", "by setting the `container_log_level=logging.DEBUG` on the `SKLearn`\n", "estimator object (after `import logging`)."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["hyperparameters = {\n", "    \"tree-n-estimators\": 42,\n", "    \"tree-max-depth\": 2,\n", "    \"tree-min-child-samples\": 1,\n", "    \"tree-boosting-type\": \"dart\"\n", "}\n", "\n", "estimator = sagemaker.sklearn.SKLearn(\n", "    image_name=str(custom_image),\n", "    source_dir=str(Path(current_folder, 'src').resolve()),\n", "    entry_point='entry_point.py',\n", "    hyperparameters=hyperparameters,\n", "    role=config.SAGEMAKER_IAM_ROLE,\n", "    train_instance_count=1,\n", "    train_instance_type=train_instance_type,\n", "    sagemaker_session=session,\n", "    output_path='s3://' + str(Path(config.S3_BUCKET, config.OUTPUTS_S3_PREFIX)),\n", "    code_location='s3://' + str(Path(config.S3_BUCKET, config.OUTPUTS_S3_PREFIX)),\n", ")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["With our estimator now initialized, we can start the Amazon SageMaker training\n", "job. Since our entry point script expects a number of data channels to be\n", "defined, we can provide them when calling `fit`. When referencing `s3://`\n", "folders, the contents of these folders will be automatically downloaded from\n", "Amazon S3 before the entry point script is run. When using local mode, it's\n", "possible to avoid this data transfer and reference local folder using the\n", "`file://` prefix instead: e.g. `{'schemas': 'file://' + str(schema_folder)}`\n", "\n", "You can expect this step to take approximately 5 minutes."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["estimator.fit({\n", "    'schemas': 's3://' + str(Path(config.S3_BUCKET, config.SCHEMAS_S3_PREFIX)),\n", "    'data_train': 's3://' + str(Path(config.S3_BUCKET, config.DATASETS_S3_PREFIX, 'data_train')),\n", "    'label_train': 's3://' + str(Path(config.S3_BUCKET, config.DATASETS_S3_PREFIX, 'label_train')),\n", "    'data_test': 's3://' + str(Path(config.S3_BUCKET, config.DATASETS_S3_PREFIX, 'data_test')),\n", "    'label_test': 's3://' + str(Path(config.S3_BUCKET, config.DATASETS_S3_PREFIX, 'label_test'))\n", "})"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Model Deployment\n", "Our Amazon SageMaker training job has now completed, and we should have a\n", "number of trained model artifacts that can be deployed. Calling `deploy` will\n", "start a container to host the model (and an instance to run the container if\n", "you're not running in local mode). Using `estimator.deploy` means that we'll\n", "use the same entry point script as used for training, but the model deployment\n", "functions (i.e. `model_fn`, `input_fn`, `predict_fn`, etc) will be used\n", "instead of the model training function (i.e. `train_fn`). We set the\n", "`EXPLAINER` environment variable to `true` to use the deployment functions\n", "that give explanations (in addition to predictions)."]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Caution**: When using local mode, you may see `docker-compose` errors if\n", "trying to deploy the estimator multiple times. You need to manually stop the\n", "original hosting container before deploying a second time. Uncomment and\n", "execute the following command to stop the original hosting container."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# !docker container stop $(docker ps -a -q --filter ancestor={config.ECR_REPOSITORY})"]}, {"cell_type": "markdown", "metadata": {}, "source": ["AWS CloudFormation will delete this endpoint (and endpoint configuration)\n", "during stack deletion if the `endpoint_name` is kept as is. You will need\n", "to manually delete the endpoint (and endpoint configuration) after stack\n", "deletion if you change this.\n", "\n", "You can expect this step to take approximately 5 minutes."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["explainer_name = \"{}-explainer\".format(config.STACK_NAME)\n", "estimator.deploy(\n", "    model_name=explainer_name,\n", "    endpoint_name=explainer_name,\n", "    instance_type=deploy_instance_type,\n", "    initial_instance_count=1,\n", "    env={'EXPLAINER': 'true'}\n", ")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["When calling our new endpoint from the notebook, we use a Amazon SageMaker SDK\n", "[`Predictor`](https://sagemaker.readthedocs.io/en/stable/predictors.html).\n", "A `Predictor` is used to send data to an endpoint (as part of a request),\n", "and interpret the response. Creating a `Predictor` does not affect the\n", "actual endpoint. Our endpoint expects to receive (and also sends) JSON\n", "formatted objects, so we create a custom `JsonPredictor`. JSON is used\n", "because it is a standard endpoint format and the endpoint response\n", "contains a nested data structure."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["explainer = predictors.JsonPredictor(explainer_name, session)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Some use-cases require seperation between predictions and explanations.\n", "As an example, you may wish to have a production endpoint for predictions\n", "that is not impacted by the creation of explanations. Uncomment the following\n", "code cell if you wish to deploy an endpoint that is dedicated to predictions.\n", "\n", "You can expect this step to take approximately 5 minutes, but it's optional."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# predictor_name = \"{}-predictor\".format(config.STACK_NAME)\n", "# estimator.deploy(\n", "#     model_name=predictor_name,\n", "#     endpoint_name=predictor_name,\n", "#     instance_type=deploy_instance_type,\n", "#     initial_instance_count=1,\n", "#     env={'EXPLAINER': 'false'}\n", "# )\n", "# predictor = predictors.JsonPredictor(predictor_name, session)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Model Explanations\n", "We can demonstrate the output of our new `explainer` endpoint with an\n", "example. One option would be to take a sample from our test set, but\n", "let's construct a sample by hand. Our example credit application is for\n", "6000 EUR and will be put towards buying a used car. You can always come\n", "back later and make changes to certain values."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["sample = {\n", "    'contact__has_telephone': False,\n", "    'credit__amount': 6000,\n", "    'credit__coapplicant': 1,\n", "    'credit__duration': 36,\n", "    'credit__guarantor': 0,\n", "    'credit__installment_rate': 3,\n", "    'credit__purpose': 'used_car',\n", "    'employment__duration': 0,\n", "    'employment__permit': 'foreign',\n", "    'employment__type': 'professional',\n", "    'finance__accounts__checking__balance': 'no_account',\n", "    'finance__accounts__savings__balance': 'low',\n", "    'finance__credits__other_banks': 0,\n", "    'finance__credits__other_stores': 0,\n", "    'finance__credits__this_bank': 1,\n", "    'finance__other_assets': 'life_insurance',\n", "    'finance__repayment_history': 'good',\n", "    'personal__num_dependents': 1,\n", "    'residence__duration': 4,\n", "    'residence__type': 'own'\n", "}"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We can call `explainer.predict` with features (for a credit application)\n", "to obtain a prediction and its associated explanation. Using\n", "`JsonPredictor`, the features will be converted from a Python list into a\n", "JSON string (using the Amazon SageMaker Python SDK's in-built\n", "`json_serializer`). Additionally, it will notify to the endpoint that the\n", "contents being sent are JSON formatted (via `content_type`), and request\n", "a JSON formatted response in return (via `accept`). And lastly, the JSON\n", "response is converted back into Python objects (using\n", "`json_deserializer`).\n", "\n", "**Caution**: the probability returned by this model has not been\n", "calibrated. When the model gives a probability of credit default of 20%,\n", "for example, this does not necessarily mean that 20% of applications with\n", "a probability of 20% resulted in credit default. Calibration is a useful\n", "property in certain circumstances, but is not required in cases where\n", "discrimination between cases of default and non-defult is sufficient.\n", "[CalibratedClassifierCV](https://scikit-learn.org/stable/modules/generated/sklearn.calibration.CalibratedClassifierCV.html)\n", "from\n", "[Scikit-learn](https://scikit-learn.org/stable/modules/calibration.html)\n", "can be used to calibrate a model. Calibration also has an impact on the\n", "explanations. Since the calibration process is typically non-linear, it\n", "breaks the additive property of Shapley Values.\n", "[`KernelExplainer`](https://shap.readthedocs.io/en/latest/) can handle\n", "this case, but is typically much slower to compute the explanations."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["output = explainer.predict(sample)\n", "prediction = output['prediction']\n", "print(\"Credit default risk: {:.2%}\".format(prediction))\n", "explanation = output['explanation']"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Visualizing Explanations\n", "Although `explanation` contains all the information required to explain\n", "the machine learning model's prediction, looking at long lists of numbers\n", "isn't especially helpful. We provide a number of visualization that\n", "clearly show which features increase and decrease the risk of credit\n", "default for an individual credit application.\n", "\n", "A waterfall chart can be used to show the cumulative effect of each\n", "feature. Starting with the baseline probability for credit defaults (at\n", "the bottom of the chart), we can see how each additional feature shifts\n", "the probability. Green arrows indicate that the feature <span\n", "style=\"color:#69AE35\">*decreased* the predicted credit default\n", "risk</span> for the individual credit application. While red arrows\n", "indicate that the feature <span style=\"color:#FF5733\">*increased* the\n", "predicted credit default risk</span> for the individual credit\n", "application. After all features have been considered, we reach the final\n", "predicted credit default risk (at the top of the chart).\n", "\n", "We're using [`bokeh`](https://docs.bokeh.org/en/latest/index.html#) for \n", "interactive charts, so let's start by calling `output_notebook` to show the\n", "plots inside the notebook."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["output_notebook()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Summary Explanation\n", "As mentioned earlier on in this notebook, our features can be grouped\n", "together into categories. We can extract the top level category for each\n", "feature, by extracting the start of the feature name before the level\n", "seperator. We use two consecutive underscores (`__`) as our level\n", "separator. Once we have the category for each feature, we can calculate\n", "the the overall effect for each category. All of this is performed in\n", "`summarize_explanation`."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["explanation_summary = visuals.summarize_explanation(explanation)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We then show the associated waterfall chart."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["x_axis_label = 'Credit Default Risk Score (%)'\n", "summary_waterfall = visuals.WaterfallChart(\n", "    baseline=explanation_summary['expected_value'],\n", "    shap_values=explanation_summary['shap_values'],\n", "    names=explanation_summary['feature_names'],\n", "    descriptions=explanation_summary['feature_descriptions'],\n", "    max_features=10,\n", "    x_axis_label=x_axis_label,\n", ")\n", "summary_waterfall.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We can see from the summary waterfall chart above that features related\n", "to finance have the largest combined effect on the credit default risk.\n", "Although features realted to finance reduce the credit default risk, the\n", "features related to employment bring the risk back up again to a certain\n", "degree."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Detailed Explanation\n", "After examining the high level explanation, we can drill down into the\n", "individual features that contribute to the credit default risk score."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["detailed_waterfall = visuals.WaterfallChart(\n", "    baseline=explanation['expected_value'],\n", "    shap_values=explanation['shap_values'],\n", "    names=explanation['feature_names'],\n", "    feature_values=explanation['feature_values'],\n", "    descriptions=explanation['feature_descriptions'],\n", "    max_features=10,\n", "    x_axis_label=x_axis_label\n", ")\n", "detailed_waterfall.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We can see from the detailed waterfall chart above that not having a\n", "checking account with the same bank indicates a lower credit default\n", "risk. Since this is an influential feature for the model but the reason\n", "for this effect is not obvious, it may warrant further investigation. We\n", "can also see that using the credit to purchase a used car is associated\n", "with a lower credit default risk too. After this we see a number of\n", "features that increase the credit default risk: a credit amount of 6000\n", "EUR, a lack of employment and a credit duration of 36 months. Another\n", "potential area for investigation, would be related to the repayment\n", "history feature. We can see that *not* having a very poor repayment\n", "history is associated with a higher credit default risk score. We may\n", "have artifacts in the datasets that caused the model to use this feature\n", "in such an unintuitive way."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Counterfactual Example\n", "And lastly, we switch the value of the checking account balance of the\n", "applicant from `no_account` to `negative`. We can then see how the\n", "overall prediction of the model changes, and also see the updated\n", "contribution of this feature. Clearly, this application has become\n", "substantially more risky."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["counter_sample = dict(sample)\n", "counter_sample['finance__accounts__checking__balance'] = 'negative'  # from 'no_account'\n", "counter_explanation = explainer.predict(counter_sample)['explanation']\n", "visuals.WaterfallChart(\n", "    baseline=counter_explanation['expected_value'],\n", "    shap_values=counter_explanation['shap_values'],\n", "    names=counter_explanation['feature_names'],\n", "    feature_values=counter_explanation['feature_values'],\n", "    descriptions=counter_explanation['feature_descriptions'],\n", "    max_features=10,\n", "    x_axis_label=x_axis_label,\n", ").show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Reports\n", "On some occasions the explanations need to be exported and shared. You can export the explanation to a HTML file. "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["report_name = 'explanation'\n", "report_filename = reports.create_report(explanation, report_name, x_axis_label)\n", "HTML(reports.report_link(report_name))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Clean Up\n", "When you've finished with this solution, make sure that you delete all\n", "unwanted AWS resources. AWS CloudFormation can be used to automatically delete\n", "all standard resources that have been created by the solution and notebook.\n", "\n", "**Caution**: You need to manually delete any extra resources that you may have\n", "created in this notebook. Some examples include, extra Amazon S3 buckets (to\n", "the solution's default bucket), extra Amazon SageMaker endpoints (using a\n", "custom name), and extra Amazon ECR repositories."]}, {"cell_type": "markdown", "metadata": {}, "source": ["You can explicitly delete the Amazon SageMaker endpoints (and endpoint\n", "configurations) using the Amazon SageMaker Python SDK, but they are also\n", "deleted when you delete the AWS CloudFormation stack if you forget."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# explainer.delete_endpoint()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# predictor.delete_endpoint()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["You can now return to AWS CloudFormation and delete the stack."]}], "metadata": {"jupytext": {"cell_metadata_filter": "-all", "main_language": "python", "notebook_metadata_filter": "-all"}, "kernelspec": {"display_name": "conda_python3", "language": "python", "name": "conda_python3"}}, "nbformat": 4, "nbformat_minor": 4}